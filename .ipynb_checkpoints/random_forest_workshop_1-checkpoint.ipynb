{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Random Forest Classifier\n",
    "### Benodigde modules:\n",
    "- pandas\n",
    "- sklearn\n",
    "- matplotlib\n",
    "- Graphviz (Pas het path hieronder aan als nodig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waarom zijn Random Forest Classifiers zo effectief?\n",
    "\n",
    "Random Forest Classifiers zijn een van meest succesvolle supervised classifying machine learning algoritmes. Dit heeft de maken met de manier waarop ze werken. Het idee achter dit algoritme is gerelateerd aan het divide and conquer principe. Door een probleem in meerdere problemen op te splitsen, kunnen veel betere resultaten behaald worden.\n",
    "\n",
    "Het divide and conquer principe kan goed uitgebeeld worden aan de hand van een spelletje. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spelregels:\n",
    "Genereer een random getal tussen de 0 en 1.\n",
    "Als het getal kleiner dan/gelijk aan 0.4 is dan verlies je het geld\n",
    "Als het getal groter dan 0.4 dan win je het geld\n",
    "\n",
    "##### Situaties:\n",
    "- 10 keer met 100 euro\n",
    "- 100 keer met 10 euro\n",
    "- 1000 keer met 1 euro\n",
    "\n",
    "In welk spelletje heb je de meeste winkans met de bovenstaande spelregels? Schrijf code om dit spelletje (x aantal keer) te kunnen spelen Simuleer elk van de drie situaties 10000 keer en plot de resultaten in een bar plot om te kijken hoevaak je in elk spelletje iets wint of verliest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove\n",
    "def win_money(play, money, repeat) -> [int]:\n",
    "    \n",
    "    total_results = []\n",
    "    \n",
    "    for r in range(repeat):\n",
    "        \n",
    "        game_result = 0\n",
    "        \n",
    "        for p in range(play):\n",
    "            \n",
    "            if random.random() <= 0.4:\n",
    "                game_result -= money\n",
    "            else:\n",
    "                game_result += money\n",
    "            \n",
    "        total_results.append(game_result)\n",
    "        \n",
    "    return total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove\n",
    "a = win_money(1, 100, 10000)\n",
    "b = win_money(10, 10, 10000)\n",
    "c = win_money(100, 1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove\n",
    "a_ = pd.Series(a).value_counts()\n",
    "b_ = pd.Series(b).value_counts()\n",
    "c_ = pd.Series(c).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to remove\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(a_.index, a_)\n",
    "plt.bar(b_.index, b_)\n",
    "plt.bar(c_.index, c_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hieronder staat een uitwerking van het voorbeeld van de presentatie\n",
    "We beginnen eerst met de dataset laden en de integers te converten naar float.\n",
    "\n",
    "Hieronder zie je hoe de train dataset eruit ziet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"dataset/fruits_train.xlsx\")\n",
    "df_test = pd.read_excel(\"dataset/fruits_test.xlsx\")\n",
    "\n",
    "train, test = df_train, df_test\n",
    "train['Diameter'] = pd.to_numeric(train['Diameter'], downcast='float')\n",
    "test['Diameter'] = pd.to_numeric(test['Diameter'], downcast='float')\n",
    "print('Number of observations in the training data:', len(train))\n",
    "print('Number of observations in the test data:',len(test))\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nu bepaal je wat de features en classes zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "features = ['Color', 'Diameter']\n",
    "classes = ['Apple','Orange','Cherries','Citrus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strings worden niet geaccepteerd door de classifier. Dit betekend dat wij moeten factorizen of dummies genereren.\n",
    "Hieronder factorizen wij de kleuren naar:\n",
    "- Red = 0\n",
    "- Orange = 1\n",
    "- Yellow = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train['Color'] = pd.factorize(train['Color'])[0]\n",
    "test['Color'] = pd.factorize(test['Color'])[0]\n",
    "# Target Feature\n",
    "y = pd.factorize(train['Label'])[0]\n",
    "\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dan definier je de RandomForestClassifier en voer je de fit uit\n",
    "In dit geval doen wij dat met 3 estimators (trees) en gebruiken entropie als meetstaaf in plaats van de gini-waarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0, n_estimators=3, criterion='entropy')\n",
    "clf.fit(train[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hier zie je welke trees eruit komen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def export_graph(estimator):\n",
    "    # Export as dot file\n",
    "    export_graphviz(estimator, out_file='tree.dot', \n",
    "                    feature_names = features,\n",
    "                    class_names = classes,\n",
    "                    rounded = True, proportion = False, \n",
    "                    precision = 2, filled = True)\n",
    "\n",
    "    # Convert to png using system command (requires Graphviz)\n",
    "    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "    # Display in jupyter notebook\n",
    "    Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "export_graph(clf.estimators_[0])\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "export_graph(clf.estimators_[1])\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "export_graph(clf.estimators_[2])\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De volgende stap is om te bepalen of het algoritme goed werkt\n",
    "De feature importances functie laat zien welke feature de grootste information gain haalt gemiddeld. Dit is interresant genoeg de diameter, ook al is de kleur discriminerender. Laat je hier dus niet door misinformeren. Verder wordt hier een score en confusion matrix (niet en wel genormaliseerd) neergezet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test[features])\n",
    "y_true = pd.factorize(test['Label'])[0]\n",
    "print(y_pred)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf.score(test[features],y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, test[features], y_true, display_labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, test[features], y_true, display_labels=classes, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier begint de workshop\n",
    "We gaan beginnen met de wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data= np.c_[wine['data'], wine['target']],\n",
    "                     columns= wine['feature_names'] + ['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# features = [df.columns[:-1]][0]\n",
    "features = ['alcohol', 'malic_acid', 'ash']\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0, n_estimators=10, criterion='entropy')\n",
    "clf.fit(df[features], df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(df[features])\n",
    "y_true = df['target']\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf.score(df[features], df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, df[features], y_true, normalize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>12.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>17.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.96</td>\n",
       "      <td>710.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>12.60</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.90</td>\n",
       "      <td>18.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.77</td>\n",
       "      <td>562.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>12.34</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.46</td>\n",
       "      <td>21.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.38</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>11.82</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.88</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.44</td>\n",
       "      <td>415.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>12.51</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.98</td>\n",
       "      <td>20.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.57</td>\n",
       "      <td>672.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>12.42</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.30</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>12.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>510.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>12.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.28</td>\n",
       "      <td>22.5</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.42</td>\n",
       "      <td>488.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>12.22</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.94</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.02</td>\n",
       "      <td>312.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>11.61</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.70</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.26</td>\n",
       "      <td>680.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "100    12.08        2.08  1.70               17.5       97.0           2.23   \n",
       "101    12.60        1.34  1.90               18.5       88.0           1.45   \n",
       "102    12.34        2.45  2.46               21.0       98.0           2.56   \n",
       "103    11.82        1.72  1.88               19.5       86.0           2.50   \n",
       "104    12.51        1.73  1.98               20.5       85.0           2.20   \n",
       "105    12.42        2.55  2.27               22.0       90.0           1.68   \n",
       "106    12.25        1.73  2.12               19.0       80.0           1.65   \n",
       "107    12.72        1.75  2.28               22.5       84.0           1.38   \n",
       "108    12.22        1.29  1.94               19.0       92.0           2.36   \n",
       "109    11.61        1.35  2.70               20.0       94.0           2.74   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "100        2.17                  0.26             1.40             3.30  1.27   \n",
       "101        1.36                  0.29             1.35             2.45  1.04   \n",
       "102        2.11                  0.34             1.31             2.80  0.80   \n",
       "103        1.64                  0.37             1.42             2.06  0.94   \n",
       "104        1.92                  0.32             1.48             2.94  1.04   \n",
       "105        1.84                  0.66             1.42             2.70  0.86   \n",
       "106        2.03                  0.37             1.63             3.40  1.00   \n",
       "107        1.76                  0.48             1.63             3.30  0.88   \n",
       "108        2.04                  0.39             2.08             2.70  0.86   \n",
       "109        2.92                  0.29             2.49             2.65  0.96   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "100                          2.96    710.0     1.0  \n",
       "101                          2.77    562.0     1.0  \n",
       "102                          3.38    438.0     1.0  \n",
       "103                          2.44    415.0     1.0  \n",
       "104                          3.57    672.0     1.0  \n",
       "105                          3.30    315.0     1.0  \n",
       "106                          3.17    510.0     1.0  \n",
       "107                          2.42    488.0     1.0  \n",
       "108                          3.02    312.0     1.0  \n",
       "109                          3.26    680.0     1.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mis = df[100:110]\n",
    "df_mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(df)\n",
    "\n",
    "\n",
    "print(imp_mean.transform(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
